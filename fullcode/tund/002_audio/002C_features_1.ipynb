{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import time\n",
    "sys.path.append('/root/code/TensorFlowASR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "import librosa\n",
    "import scipy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import nlpaug.augmenter.audio as naa\n",
    "from augmentation import AudioTFRecordDataset\n",
    "from tensorflow_asr.featurizers.speech_featurizers import NumpySpeechFeaturizer, TFSpeechFeaturizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = 16000\n",
    "frame_length = 25 * 16000 // 1000\n",
    "frame_step = 10 * 16000 // 1000\n",
    "num_feature_bins = 80\n",
    "nfft = 2 ** (frame_length - 1).bit_length()\n",
    "\n",
    "def get_audio_noise_pair(dataset_iter):\n",
    "    audio, noise = next(dataset_iter)\n",
    "    return audio.numpy().squeeze(), noise.numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AudioTFRecordDataset(tfrecords_dir=\"/root/datasets/libri_asr/all-tfrecords\",\n",
    "                               noise_tfrecords_dir=\"/root/datasets/libri_asr/all-tfrecords\",\n",
    "                               shuffle=False).create()\n",
    "dataset_iter = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, noise = get_audio_noise_pair(dataset_iter)\n",
    "audio, noise = get_audio_noise_pair(dataset_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = audio[: 3 * sampling_rate]\n",
    "IPython.display.Audio(data=signal, rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 4))\n",
    "librosa.display.waveplot(signal, sr=sampling_rate, max_points=50000.0, x_axis='time', offset=0.0, max_sr=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_signal = signal / np.max(np.abs(signal) + 1e-6)\n",
    "plt.figure(figsize=(15,4))\n",
    "librosa.display.waveplot(normalized_signal, sr=sampling_rate, max_points=50000.0, x_axis='time', offset=0.0, max_sr=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(data=normalized_signal, rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-emphasis\n",
    "\n",
    "A pre-emphasis filter is useful in several ways:\n",
    "\n",
    "1. balance the frequency spectrum since high frequencies usually have smaller magnitudes compared to lower frequencies, \n",
    "2. avoid numerical problems during the Fourier transform operation and \n",
    "3. may also improve the Signal-to-Noise Ratio (SNR).\n",
    "\n",
    "The pre-emphasis filter can be applied to a signal x using the first order filter in the following equation:\n",
    "\n",
    "$y(t)=x(t)−αx(t−1)$\n",
    "\n",
    "The typical values for the filter coefficient are $0.95$ or $0.97$\n",
    "\n",
    "Pre-emphasis has a modest effect in modern systems, mainly because **most of the motivations for the pre-emphasis filter can be achieved using mean normalization** (discussed later in this post) except for avoiding the Fourier transform **numerical issues which should not be a problem in modern FFT implementations.**[1]\n",
    "\n",
    "The signal after pre-emphasis has the following form in the time domain:\n",
    "\n",
    "[1] https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_emphasis = 0.95\n",
    "emphasized_signal = np.append(normalized_signal[0], normalized_signal[1:] - pre_emphasis * normalized_signal[:-1])\n",
    "plt.figure(figsize=(15,4))\n",
    "librosa.display.waveplot(emphasized_signal, sr=sampling_rate, max_points=50000.0, x_axis='time', offset=0.0, max_sr=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(data=emphasized_signal, rate=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectrogram\n",
    "\n",
    "This is motivated by the human cochlea (an organ in the ear) which vibrates at different spots depending on the frequency of the incoming sounds. **Depending on the location in the cochlea that vibrates (which wobbles small hairs), different nerves fire informing the brain that certain frequencies are present**. Our periodogram estimate performs a similar job for us, identifying which frequencies are present in the frame. In particular, $spectrogram(t, \\omega) = \\vert STFT(t, \\omega)\\vert^2$ is the squared of amplitude at time $t$ and frequency $\\omega$ [1]\n",
    "\n",
    "[1] http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_stft = np.square(np.abs(librosa.stft(emphasized_signal, n_fft=nfft, hop_length=frame_step, win_length=frame_length, center=False, window=\"hann\")))\n",
    "spec = librosa.power_to_db(signal_stft, ref=1.0, amin=1e-10, top_db=80.0)\n",
    "# spec = signal_stft\n",
    "plt.figure(figsize=(15,4))\n",
    "librosa.display.specshow(spec, x_axis='time', y_axis='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec.shape  # freq x time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(spec[::-1, :], aspect='auto')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use `stft` in `scipy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signal_stft_1, _, _ = scipy.signal.stft(emphasized_signal, nfft=nfft, fs=1, window='hann', nperseg=frame_length, noverlap=frame_step, padded=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(signal_stft.shape)\n",
    "# print(signal_stft_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If we don't (log) rescale spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,4))\n",
    "librosa.display.specshow(signal_stft, x_axis='time', y_axis='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(signal_stft.shape)\n",
    "print(spec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use different number of frequency bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames = (len(emphasized_signal) - frame_length) / frame_step\n",
    "print(num_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_stft_1 = np.square(np.abs(librosa.stft(emphasized_signal, n_fft=frame_length, hop_length=frame_step, win_length=frame_length, center=False, window=\"hann\")))\n",
    "spec_1 = librosa.power_to_db(signal_stft_1, ref=1.0, amin=1e-10, top_db=80.0)\n",
    "plt.figure(figsize=(15,4))\n",
    "librosa.display.specshow(spec_1, x_axis='time', y_axis='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(signal_stft_1.shape)\n",
    "print(spec_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Mel Spectrogram\n",
    "\n",
    "The periodogram spectral estimate still contains a lot of information not required for Automatic Speech Recognition (ASR). In particular the cochlea can not discern the difference between two closely spaced frequencies. This effect becomes more pronounced as the frequencies increase. In other words, **humans are much better at discerning small changes in pitch at low frequencies than they are at high frequencies. For this reason we take clumps of periodogram bins and sum them up to get an idea of how much energy exists in various frequency regions.** This is performed by our Mel filterbank: the first filter is very narrow and gives an indication of how much energy exists near 0 Hertz. As the frequencies get higher our filters get wider as we become less concerned about variations. We are only interested in roughly how much energy occurs at each spot. The Mel scale tells us exactly how to space our filterbanks and how wide to make them. In particular, the formula for converting from frequency to Mel scale is:[1]\n",
    "\n",
    "$M(f) = 1125\\ln(1 + f/700)$\n",
    "\n",
    "To go from Mels back to frequency:\n",
    "\n",
    "$M^{-1}(m) = 700(\\exp(m/1125) - 1)$\n",
    "\n",
    "Once we have the filterbank energies, we take the logarithm of them. This is also motivated by human hearing: **we don't hear loudness on a linear scale. Generally to double the percieved volume of a sound we need to put 8 times as much energy into it**. This means that large variations in energy may not sound all that different if the sound is loud to begin with. This compression operation makes our features match more closely what humans actually hear. Why the logarithm and not a cube root? The logarithm allows us to use cepstral mean subtraction, which is a channel normalisation technique. \n",
    "\n",
    "With librosa, we can simply compute a linear transformation matrix to project FFT bins onto Mel-frequency bins.\n",
    "\n",
    "[1] http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce a linear transformation matrix to project FFT bins onto Mel-frequency bins\n",
    "# the matrix has shape=(n_mels, 1 + nfft/2)\n",
    "mel = librosa.filters.mel(sampling_rate, nfft, n_mels=num_feature_bins, fmin=0.0, fmax=int(sampling_rate / 2))\n",
    "# sum up power in each Mel-frequency bins\n",
    "mel_spec = np.dot(signal_stft.T, mel.T)\n",
    "# take log of the amplitude\n",
    "mel_spec = librosa.power_to_db(mel_spec, ref=1.0, amin=1e-10, top_db=80.0)\n",
    "# visualize\n",
    "plt.figure(figsize=(15,4))\n",
    "librosa.display.specshow(mel_spec.T, x_axis='time', y_axis='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way:\n",
    "# mel_spec = librosa.feature.melspectrogram(signal, sr=sampling_rate, n_fft=nfft, hop_length=frame_step, n_mels=num_feature_bins, center=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(signal_stft.shape)  # freq bins x time\n",
    "print(mel.shape)  # Mel-banks x freq bins\n",
    "print(mel_spec.shape)  # time x Mel-banks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mel_spec.T[::-1, :], aspect='auto')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To balance the spectrum and improve the Signal-to-Noise (SNR), we can simply subtract the mean of each coefficient from all frames.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(np.log(mel_spec + 1e-8).ravel(), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spec_mean = np.mean(mel_spec)\n",
    "mel_spec_std = np.std(mel_spec) + 1e-9\n",
    "mel_spec_norm = (mel_spec - mel_spec_mean) / mel_spec_std\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "librosa.display.specshow(mel_spec_norm, x_axis='time', y_axis='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Padding Mel_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_input_length = 400\n",
    "mel_spec_norm_padded = np.vstack([mel_spec_norm, np.zeros([target_input_length - mel_spec_norm.shape[0], mel_spec_norm.shape[1]])])\n",
    "print(mel_spec_norm_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spec_norm_padded[-1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 4))\n",
    "librosa.display.specshow(mel_spec_norm_padded.T, x_axis='time', y_axis='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.describe(mel_spec_norm.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.describe(mel_spec_norm_padded.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If we pad signal before extracting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = audio[: 3 * sampling_rate]\n",
    "IPython.display.Audio(data=signal, rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_padded = int(target_input_length * frame_step + frame_length) - len(signal)\n",
    "signal_padded = np.concatenate([signal, np.zeros(num_padded)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(data=signal_padded, rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_signal_padded = signal_padded / np.max(np.abs(signal_padded) + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_emphasis = 0.95\n",
    "emphasized_signal_padded = np.append(normalized_signal_padded[0], normalized_signal_padded[1:] - pre_emphasis * normalized_signal_padded[:-1])\n",
    "plt.figure(figsize=(15,4))\n",
    "librosa.display.waveplot(emphasized_signal_padded, sr=sampling_rate, max_points=50000.0, x_axis='time', offset=0.0, max_sr=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(data=emphasized_signal_padded, rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_stft_padded = np.square(np.abs(librosa.stft(emphasized_signal_padded, n_fft=nfft, hop_length=frame_step, win_length=frame_length, center=False, window=\"hann\")))\n",
    "spec_padded = librosa.power_to_db(signal_stft_padded, ref=1.0, amin=1e-10, top_db=80.0)\n",
    "plt.figure(figsize=(15,4))\n",
    "librosa.display.specshow(spec_padded, x_axis='time', y_axis='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce a linear transformation matrix to project FFT bins onto Mel-frequency bins\n",
    "# the matrix has shape=(n_mels, 1 + nfft/2)\n",
    "mel_padded = librosa.filters.mel(sampling_rate, nfft, n_mels=num_feature_bins, fmin=0.0, fmax=int(sampling_rate / 2))\n",
    "# sum up power in each Mel-frequency bins\n",
    "mel_spec_padded = np.dot(signal_stft_padded.T, mel_padded.T)\n",
    "# take log of the amplitude\n",
    "mel_spec_padded = librosa.power_to_db(mel_spec_padded, ref=1.0, amin=1e-10, top_db=80.0)\n",
    "# visualize\n",
    "plt.figure(figsize=(15,4))\n",
    "librosa.display.specshow(mel_spec_padded.T, x_axis='time', y_axis='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(signal_stft_padded.shape)\n",
    "print(mel_padded.shape)\n",
    "print(mel_spec_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spec_padded[-1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spec_mean = np.mean(mel_spec_padded)\n",
    "mel_spec_std = np.std(mel_spec_padded) + 1e-9\n",
    "mel_spec_norm_padded = (mel_spec_padded - mel_spec_mean) / mel_spec_std\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "librosa.display.specshow(mel_spec_norm_padded.T, x_axis='time', y_axis='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spec_norm_padded[-1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.describe(mel_spec_norm_padded.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mel-Frequency Cepstral Coefficients (MFCC)\n",
    "\n",
    "The final step is to compute the Discrete Cosine Transform of the log filterbank energies. There are 2 main reasons this is performed.[1]\n",
    "\n",
    "1. **Our filterbanks are all overlapping, the filterbank energies are quite correlated with each other. The DCT decorrelates the energies which means diagonal covariance matrices can be used to model the features in e.g. a HMM classifier.**\n",
    "\n",
    "\n",
    "2. The higher DCT coefficients represent fast changes in the filterbank energies and it turns out that these fast changes actually degrade ASR performance, so we get a small improvement by dropping them. Therefore, only 12 of the 26 DCT coefficients are kept.\n",
    "\n",
    "Regarding the first reason, one might question if MFCCs are still the right choice for Deep Learning models given that deep neural networks are less susceptible to highly correlated input and therefore **the Discrete Cosine Transform (DCT) is no longer a necessary step**. It is beneficial to note that Discrete Cosine Transform (DCT) is a linear transformation, and therefore undesirable as it discards some information in speech signals which are highly non-linear.\n",
    "\n",
    "It is interesting to note that **all steps needed to compute filter banks were motivated by the nature of the speech signal andthe human perception of such signals.** On the contrary, the **extra steps needed to compute MFCCs were motivated by thelimitation of some machine learning algorithms**.[2] The Discrete Cosine Transform (DCT) was needed to decorrelate filter bankcoefficients, a process also referred to as whitening. In particular, **MFCCs were very popular when Gaussian Mixture Models -Hidden Markov Models (GMMs-HMMs) were very popular** and together, MFCCs and GMMs-HMMs co-evolved to be the standard way of doing Automatic Speech Recognition (ASR) \n",
    "\n",
    "[1] http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/\n",
    "\n",
    "[2] https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce a linear transformation matrix to project FFT bins onto Mel-frequency bins\n",
    "# the matrix has shape=(n_mels, 1 + nfft/2)\n",
    "mel = librosa.filters.mel(sampling_rate, nfft, n_mels=num_feature_bins, fmin=0.0, fmax=int(sampling_rate / 2))\n",
    "# sum up power in each Mel-frequency bins\n",
    "mel_spec = np.dot(signal_stft.T, mel.T)\n",
    "# take log of the amplitude\n",
    "mel_spec = librosa.power_to_db(mel_spec, ref=1.0, amin=1e-10, top_db=80.0)\n",
    "# compute the DCT of the log filterbank energies\n",
    "mfcc = librosa.feature.mfcc(sr=sampling_rate,\n",
    "                            S=mel_spec,\n",
    "                            n_mfcc=num_feature_bins)\n",
    "# visualize\n",
    "plt.figure(figsize=(15, 4))\n",
    "librosa.display.specshow(mfcc, x_axis='time', y_axis='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Time Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "audios = [None] * N\n",
    "noises = [None] * N\n",
    "\n",
    "for i in range(N):\n",
    "    audios[i], noises[i] = get_audio_noise_pair(dataset_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_config = {\n",
    "    \"sample_rate\": 16000,\n",
    "    \"frame_ms\": 25,\n",
    "    \"stride_ms\": 10,\n",
    "    \"num_feature_bins\": 80,\n",
    "    \"feature_type\": \"log_mel_spectrogram\",\n",
    "    \"preemphasis\": 0.97,\n",
    "    \"delta\": False,\n",
    "    \"delta_delta\": False,\n",
    "    \"pitch\": False,\n",
    "    \"normalize_signal\": True,\n",
    "    \"normalize_feature\": True,\n",
    "    \"normalize_per_feature\": False    \n",
    "}\n",
    "np_featurizer = NumpySpeechFeaturizer(speech_config)\n",
    "tf_featurizer = TFSpeechFeaturizer(speech_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Using Numpy/Librosa\n",
    "\n",
    "Takes on average 62.5ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t1 = []\n",
    "time0 = time.time()\n",
    "for i in range(N):\n",
    "    start = time.time()\n",
    "    features = np_featurizer.extract(audios[i])\n",
    "    duration = time.time() - start\n",
    "    t1.append(duration * 1000)\n",
    "    print(f\"{i}-th audio took {duration * 1000}ms\")\n",
    "    \n",
    "print(\"{} batches took {}ms\".format(N, (time.time() - time0) * 1000))\n",
    "print(\"Each pair took on average {}ms\".format(np.array(t1).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Using TF Data\n",
    "\n",
    "Takes on average 28.8ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t2 = []\n",
    "time0 = time.time()\n",
    "for i in range(N):\n",
    "    start = time.time()\n",
    "    features = tf_featurizer.extract(audios[i])\n",
    "    duration = time.time() - start\n",
    "    t2.append(duration * 1000)\n",
    "    print(f\"{i}-th audio took {duration * 1000}ms\")\n",
    "    \n",
    "print(\"{} batches took {}ms\".format(N, (time.time() - time0) * 1000))\n",
    "print(\"Each pair took on average {}ms\".format(np.array(t2).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
